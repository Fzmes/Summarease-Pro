import torch
from transformers import (
    pipeline,
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    MBart50TokenizerFast,
    MBartForConditionalGeneration,
    MarianMTModel,
    MarianTokenizer,
    LEDTokenizer,
    LEDForConditionalGeneration
)
import logging
from typing import Dict, List, Optional, Tuple
import warnings
import requests
from bs4 import BeautifulSoup
import re
import gc
from collections import Counter

warnings.filterwarnings("ignore")

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ScientificArticleProcessor:
    """Processeur sp√©cialis√© pour articles scientifiques et documents longs (1-90 pages)"""

    def __init__(self, device: str = "auto"):
        self.device = self._setup_device(device)
        self.models_loaded = False

        # Mapping √©tendu des langues scientifiques
        self.language_codes = {
            "fran√ßais": "fr", "fr": "fr", "french": "fr",
            "anglais": "en", "en": "en", "english": "en",
            "espagnol": "es", "es": "es", "spanish": "es",
            "allemand": "de", "de": "de", "german": "de",
            "italien": "it", "it": "it", "italian": "it",
            "portugais": "pt", "pt": "pt", "portuguese": "pt",
            "arabe": "ar", "ar": "ar", "arabic": "ar",
            "russe": "ru", "ru": "ru", "russian": "ru",
            "chinois": "zh", "zh": "zh", "chinese": "zh",
            "japonais": "ja", "ja": "ja", "japanese": "ja"
        }

        # Configuration des mod√®les sp√©cialis√©s pour documents longs
        self.model_configs = {
            # Mod√®les de r√©sum√© pour contexte long
            "scientific_led": {
                "name": "allenai/led-base-16384",
                "type": "scientific_summary",
                "max_tokens": 16384,
                "priority": 1
            },
            "scientific_bart": {
                "name": "facebook/bart-large-cnn",
                "type": "summary",
                "max_tokens": 1024,
                "priority": 2
            },
            "scientific_mt5": {
                "name": "google/mt5-small",
                "type": "multilingual_summary",
                "max_tokens": 512,
                "priority": 3
            },
            # Mod√®les de traduction scientifique
            "translate_m2m": {
                "name": "facebook/m2m100_418M",
                "type": "translation",
                "max_tokens": 512,
                "priority": 1
            },
            "translate_mbart": {
                "name": "facebook/mbart-large-50-many-to-many-mmt",
                "type": "translation",
                "max_tokens": 512,
                "priority": 2
            }
        }

        self.loaded_models = {}
        self.loaded_tokenizers = {}
        
        self._load_scientific_models()

    def _setup_device(self, device: str) -> torch.device:
        """Configure le device avec optimisation pour longs documents"""
        if device == "auto":
            if torch.cuda.is_available():
                return torch.device("cuda")
            elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
                return torch.device("mps")
            else:
                return torch.device("cpu")
        return torch.device(device)

    def _load_scientific_models(self):
        """Charge les mod√®les sp√©cialis√©s pour articles scientifiques"""
        try:
            logger.info("üî¨ Chargement des mod√®les scientifiques pour documents longs...")

            # Mod√®le LED pour contexte long (16K tokens)
            try:
                self.loaded_tokenizers["scientific_led"] = LEDTokenizer.from_pretrained("allenai/led-base-16384")
                self.loaded_models["scientific_led"] = LEDForConditionalGeneration.from_pretrained(
                    "allenai/led-base-16384",
                    torch_dtype=torch.float16 if self.device.type == "cuda" else torch.float32
                ).to(self.device)
                logger.info("‚úÖ Mod√®le LED (16K tokens) charg√© pour documents longs")
            except Exception as e:
                logger.warning(f"Mod√®le LED non disponible: {e}")

            # Mod√®les de r√©sum√© standard
            try:
                self.loaded_models["scientific_bart"] = pipeline(
                    "summarization",
                    model="facebook/bart-large-cnn",
                    tokenizer="facebook/bart-large-cnn",
                    device=self.device,
                    torch_dtype=torch.float16 if self.device.type == "cuda" else torch.float32
                )
                logger.info("‚úÖ Mod√®le BART scientifique charg√©")
            except Exception as e:
                logger.warning(f"Mod√®le BART non disponible: {e}")

            # Mod√®le multilingue
            try:
                self.loaded_models["scientific_mt5"] = pipeline(
                    "summarization",
                    model="google/mt5-small",
                    tokenizer="google/mt5-small",
                    device=self.device
                )
                logger.info("‚úÖ Mod√®le mT5 multilingue charg√©")
            except Exception as e:
                logger.warning(f"Mod√®le mT5 non disponible: {e}")

            # Mod√®les de traduction
            try:
                self.loaded_tokenizers["translate_m2m"] = AutoTokenizer.from_pretrained("facebook/m2m100_418M")
                self.loaded_models["translate_m2m"] = AutoModelForSeq2SeqLM.from_pretrained(
                    "facebook/m2m100_418M",
                    torch_dtype=torch.float16 if self.device.type == "cuda" else torch.float32
                ).to(self.device)
                logger.info("‚úÖ Mod√®le M2M100 de traduction charg√©")
            except Exception as e:
                logger.warning(f"Mod√®le M2M100 non disponible: {e}")

            self.models_loaded = len(self.loaded_models) > 0
            logger.info("‚úÖ Mod√®les scientifiques charg√©s avec succ√®s!")

        except Exception as e:
            logger.error(f"‚ùå Erreur lors du chargement des mod√®les scientifiques: {e}")
            self.models_loaded = False

    def preprocess_scientific_text(self, text: str) -> Dict:
        """Pr√©traitement avanc√© pour articles scientifiques"""
        # Nettoyage du texte
        text = re.sub(r'\s+', ' ', text)  # Espaces multiples
        text = re.sub(r'\[\d+\]', '', text)  # R√©f√©rences [1], [2], etc.
        text = re.sub(r'\([^)]*\)', '', text)  # Parenth√®ses
        text = re.sub(r'Figure\s+\d+[:\-]\s*', '', text, flags=re.IGNORECASE)  # R√©f√©rences aux figures
        
        # D√©tection des sections scientifiques
        sections = self._extract_scientific_sections(text)
        
        # M√©triques
        word_count = len(text.split())
        char_count = len(text)
        estimated_pages = max(1, word_count // 500)  # ~500 mots par page
        
        return {
            "cleaned_text": text.strip(),
            "sections": sections,
            "metrics": {
                "word_count": word_count,
                "char_count": char_count,
                "estimated_pages": estimated_pages,
                "is_long_document": word_count > 3000
            },
            "language": self._detect_scientific_language(text)
        }

    def _extract_scientific_sections(self, text: str) -> Dict:
        """Extrait les sections typiques d'un article scientifique"""
        sections = {
            "abstract": "",
            "introduction": "",
            "methodology": "",
            "results": "",
            "discussion": "",
            "conclusion": "",
            "references": ""
        }
        
        # Patterns pour sections scientifiques
        patterns = {
            "abstract": r'(abstract|summary|r√©sum√©)[\s:\-]*\n*(.*?)(?=\n\s*\n|\n\s*1\.|\n\s*introduction|$)',
            "introduction": r'(1\.)?\s*introduction[\s:\-]*\n*(.*?)(?=\n\s*\n|\n\s*2\.|\n\s*method|$)',
            "methodology": r'(2\.)?\s*(method|methodology|materials and methods)[\s:\-]*\n*(.*?)(?=\n\s*\n|\n\s*3\.|\n\s*results|$)',
            "results": r'(3\.)?\s*(results|findings)[\s:\-]*\n*(.*?)(?=\n\s*\n|\n\s*4\.|\n\s*discussion|$)',
            "discussion": r'(4\.)?\s*discussion[\s:\-]*\n*(.*?)(?=\n\s*\n|\n\s*5\.|\n\s*conclusion|$)',
            "conclusion": r'(5\.)?\s*conclusion[\s:\-]*\n*(.*?)(?=\n\s*\n|\n\s*references|$)',
            "references": r'(references|bibliography)[\s:\-]*\n*(.*?)(?=$)'
        }
        
        for section, pattern in patterns.items():
            match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
            if match:
                sections[section] = match.group(2 if section == "abstract" else 3 if section in ["methodology", "results"] else 2).strip()
                
        return sections

    def _detect_scientific_language(self, text: str) -> str:
        """D√©tection de la langue bas√©e sur le vocabulaire scientifique"""
        sample = text[:2000].lower()
        
        language_keywords = {
            "en": ["the", "this", "study", "research", "method", "results", "conclusion", "analysis"],
            "fr": ["√©tude", "recherche", "m√©thode", "r√©sultats", "conclusion", "analyse", "cette"],
            "es": ["estudio", "investigaci√≥n", "m√©todo", "resultados", "conclusi√≥n", "an√°lisis", "este"],
            "de": ["studie", "forschung", "methode", "ergebnisse", "schlussfolgerung", "analyse", "diese"]
        }
        
        scores = {}
        for lang, keywords in language_keywords.items():
            score = sum(1 for keyword in keywords if keyword in sample)
            scores[lang] = score
            
        detected = max(scores.items(), key=lambda x: x[1])
        lang_map = {"en": "anglais", "fr": "fran√ßais", "es": "espagnol", "de": "allemand"}
        return lang_map.get(detected[0], "anglais")

    def chunk_scientific_text(self, text: str, chunk_size: int = 4000, overlap: int = 200) -> List[str]:
        """D√©coupe le texte scientifique en chunks intelligents avec chevauchement"""
        sentences = re.split(r'(?<=[.!?])\s+', text)
        
        chunks = []
        current_chunk = []
        current_length = 0
        
        for sentence in sentences:
            sentence_length = len(sentence.split())
            
            if current_length + sentence_length > chunk_size and current_chunk:
                # Sauvegarder le chunk actuel
                chunks.append(' '.join(current_chunk))
                
                # Cr√©er un nouveau chunk avec chevauchement
                overlap_sentences = current_chunk[-max(1, len(current_chunk) // 4):]
                current_chunk = overlap_sentences + [sentence]
                current_length = sum(len(s.split()) for s in overlap_sentences) + sentence_length
            else:
                current_chunk.append(sentence)
                current_length += sentence_length
                
        # Ajouter le dernier chunk
        if current_chunk:
            chunks.append(' '.join(current_chunk))
            
        logger.info(f"üìÑ Texte d√©coup√© en {len(chunks)} chunks")
        return chunks

    def summarize_long_article(self, text: str, language: str = "anglais", 
                             summary_type: str = "structured") -> Dict:
        """
        R√©sum√© sp√©cialis√© pour articles scientifiques longs
        Types: structured, abstract, key_points, comprehensive
        """
        if not self.models_loaded:
            raise RuntimeError("Les mod√®les ne sont pas charg√©s")

        try:
            # Pr√©traitement et analyse
            processed = self.preprocess_scientific_text(text)
            logger.info(f"üìä Document analys√©: {processed['metrics']}")

            # Strat√©gie bas√©e sur la longueur
            if processed["metrics"]["word_count"] > 10000:
                return self._summarize_very_long_article(processed, language, summary_type)
            elif processed["metrics"]["word_count"] > 3000:
                return self._summarize_long_article(processed, language, summary_type)
            else:
                return self._summarize_medium_article(processed, language, summary_type)

        except Exception as e:
            logger.error(f"‚ùå Erreur r√©sum√© scientifique: {e}")
            return self._fallback_scientific_summary(text, language)

    def _summarize_very_long_article(self, processed: Dict, language: str, summary_type: str) -> Dict:
        """R√©sum√© d'articles tr√®s longs (>10,000 mots)"""
        text = processed["cleaned_text"]
        chunks = self.chunk_scientific_text(text, chunk_size=3000, overlap=300)
        
        chunk_summaries = []
        total_chunks = len(chunks)
        
        for i, chunk in enumerate(chunks):
            logger.info(f"üìù Traitement chunk {i+1}/{total_chunks}")
            
            try:
                if "scientific_led" in self.loaded_models:
                    summary = self._summarize_with_led(chunk)
                else:
                    summary = self._summarize_with_bart(chunk)
                    
                chunk_summaries.append(summary)
                
                # Lib√©ration m√©moire p√©riodique
                if i % 2 == 0:
                    self.cleanup_memory()
                    
            except Exception as e:
                logger.warning(f"Chunk {i+1} √©chou√©: {e}")
                continue

        # Combinaison hi√©rarchique
        if len(chunk_summaries) > 1:
            combined_text = ' '.join(chunk_summaries)
            final_summary = self._create_structured_summary(combined_text, summary_type)
        else:
            final_summary = chunk_summaries[0] if chunk_summaries else "R√©sum√© non disponible"

        return {
            "summary": final_summary,
            "sections_analyzed": [k for k, v in processed["sections"].items() if v],
            "original_metrics": processed["metrics"],
            "summary_type": summary_type,
            "chunks_processed": len(chunks),
            "processing_strategy": "very_long_hierarchical"
        }

    def _summarize_long_article(self, processed: Dict, language: str, summary_type: str) -> Dict:
        """R√©sum√© d'articles longs (3,000-10,000 mots)"""
        text = processed["cleaned_text"]
        
        # Utiliser LED si disponible pour le contexte long
        if "scientific_led" in self.loaded_models:
            summary = self._summarize_with_led(text)
        else:
            # Fallback: r√©sum√© par sections
            summary = self._summarize_by_sections(processed)
        
        structured_summary = self._create_structured_summary(summary, summary_type)

        return {
            "summary": structured_summary,
            "sections_analyzed": [k for k, v in processed["sections"].items() if v],
            "original_metrics": processed["metrics"],
            "summary_type": summary_type,
            "processing_strategy": "long_direct"
        }

    def _summarize_medium_article(self, processed: Dict, language: str, summary_type: str) -> Dict:
        """R√©sum√© d'articles de longueur moyenne"""
        text = processed["cleaned_text"]
        
        if language.lower() in ["anglais", "english"] and "scientific_bart" in self.loaded_models:
            summary = self._summarize_with_bart(text)
        elif "scientific_mt5" in self.loaded_models:
            summary = self._summarize_with_mt5(text, language)
        else:
            summary = self._summarize_with_bart(text)
        
        structured_summary = self._create_structured_summary(summary, summary_type)

        return {
            "summary": structured_summary,
            "sections_analyzed": [k for k, v in processed["sections"].items() if v],
            "original_metrics": processed["metrics"],
            "summary_type": summary_type,
            "processing_strategy": "medium_direct"
        }

    def _summarize_with_led(self, text: str) -> str:
        """Utilise LED pour les longs contextes (16K tokens)"""
        tokenizer = self.loaded_tokenizers["scientific_led"]
        model = self.loaded_models["scientific_led"]
        
        inputs = tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            max_length=8192  # LED peut g√©rer jusqu'√† 16384
        ).to(self.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=512,
                min_length=200,
                num_beams=4,
                early_stopping=True,
                no_repeat_ngram_size=3
            )
            
        return tokenizer.decode(outputs[0], skip_special_tokens=True)

    def _summarize_with_bart(self, text: str) -> str:
        """Utilise BART pour r√©sum√© scientifique"""
        model = self.loaded_models["scientific_bart"]
        
        result = model(
            text,
            max_length=300,
            min_length=100,
            do_sample=False
        )
        return result[0]['summary_text']

    def _summarize_with_mt5(self, text: str, language: str) -> str:
        """Utilise mT5 pour r√©sum√© multilingue"""
        model = self.loaded_models["scientific_mt5"]
        
        # Ajouter le pr√©fixe de langue si n√©cessaire
        if hasattr(model, 'model') and hasattr(model.model, 'config'):
            if 'mt5' in model.model.config.name_or_path.lower():
                text = f"summarize: {text}"

        result = model(
            text,
            max_length=300,
            min_length=100,
            do_sample=False
        )
        return result[0]['summary_text']

    def _summarize_by_sections(self, processed: Dict) -> str:
        """R√©sum√© bas√© sur l'extraction des sections"""
        sections = processed["sections"]
        summary_parts = []
        
        # Priorit√© des sections pour le r√©sum√©
        priority_sections = ["abstract", "conclusion", "results", "introduction"]
        
        for section in priority_sections:
            if sections[section] and len(sections[section].split()) > 10:
                summary_parts.append(f"{section.upper()}: {sections[section]}")
        
        if summary_parts:
            return "\n\n".join(summary_parts[:3])  # Limiter √† 3 sections
        else:
            # Fallback: premi√®res et derni√®res phrases
            sentences = processed["cleaned_text"].split('. ')
            if len(sentences) > 6:
                return '. '.join(sentences[:3] + sentences[-3:]) + '.'
            else:
                return processed["cleaned_text"]

    def _create_structured_summary(self, summary: str, summary_type: str) -> str:
        """Cr√©e un r√©sum√© structur√© selon le type demand√©"""
        if summary_type == "structured":
            return f"üìä R√âSUM√â STRUCTUR√â\n\n{summary}"
        elif summary_type == "abstract":
            return f"üìã ABSTRACT\n\n{summary}"
        elif summary_type == "key_points":
            # Extraction des points cl√©s
            sentences = summary.split('. ')
            key_points = [s.strip() for s in sentences if len(s.split()) > 5]
            return "üéØ POINTS CL√âS\n\n‚Ä¢ " + "\n‚Ä¢ ".join(key_points[:7])
        elif summary_type == "comprehensive":
            return f"üîç ANALYSE COMPL√àTE\n\n{summary}"
        else:
            return summary

    def translate_scientific_text(self, text: str, source_lang: str, target_lang: str) -> str:
        """Traduction sp√©cialis√©e pour texte scientifique"""
        if not self.models_loaded:
            raise RuntimeError("Les mod√®les ne sont pas charg√©s")

        try:
            # Pour les textes tr√®s longs, d√©coupage
            if len(text.split()) > 2000:
                return self._translate_long_scientific_text(text, source_lang, target_lang)
            else:
                return self._translate_short_scientific_text(text, source_lang, target_lang)

        except Exception as e:
            logger.error(f"‚ùå Erreur traduction scientifique: {e}")
            return self._fallback_translation(text, target_lang)

    def _translate_long_scientific_text(self, text: str, source_lang: str, target_lang: str) -> str:
        """Traduction de textes scientifiques tr√®s longs"""
        chunks = self.chunk_scientific_text(text, chunk_size=1500, overlap=100)
        translated_chunks = []
        
        for i, chunk in enumerate(chunks):
            logger.info(f"üåç Traduction chunk {i+1}/{len(chunks)}")
            try:
                translated_chunk = self._translate_short_scientific_text(chunk, source_lang, target_lang)
                translated_chunks.append(translated_chunk)
                
                # Lib√©ration m√©moire
                if i % 3 == 0:
                    self.cleanup_memory()
                    
            except Exception as e:
                logger.warning(f"Traduction chunk {i+1} √©chou√©e: {e}")
                translated_chunks.append(f"[Erreur de traduction: {str(e)}]")
        
        return ' '.join(translated_chunks)

    def _translate_short_scientific_text(self, text: str, source_lang: str, target_lang: str) -> str:
        """Traduction de textes scientifiques courts"""
        if "translate_m2m" not in self.loaded_models:
            raise ValueError("Mod√®le de traduction non disponible")

        model = self.loaded_models["translate_m2m"]
        tokenizer = self.loaded_tokenizers["translate_m2m"]
        
        src_code = self.language_codes.get(source_lang.lower(), "en")
        tgt_code = self.language_codes.get(target_lang.lower(), "en")
        
        tokenizer.src_lang = src_code
        
        inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512).to(self.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                forced_bos_token_id=tokenizer.get_lang_id(tgt_code),
                max_length=512,
                num_beams=4,
                early_stopping=True
            )
            
        return tokenizer.decode(outputs[0], skip_special_tokens=True)

    def extract_scientific_metadata(self, text: str) -> Dict:
        """Extrait les m√©tadonn√©es d'un article scientifique"""
        processed = self.preprocess_scientific_text(text)
        
        # D√©tection des domaines scientifiques
        domains = self._detect_scientific_domains(text)
        
        # Mots cl√©s (simplifi√©)
        words = re.findall(r'\b[a-zA-Z]{5,}\b', text.lower())
        word_freq = Counter(words)
        keywords = [word for word, count in word_freq.most_common(15) if count > 2 and len(word) > 4]
        
        return {
            "domains": domains,
            "keywords": keywords[:10],
            "sections_present": [section for section, content in processed["sections"].items() if content],
            "language": processed["language"],
            **processed["metrics"]
        }

    def _detect_scientific_domains(self, text: str) -> List[str]:
        """D√©tection des domaines scientifiques bas√©e sur le vocabulaire"""
        text_lower = text.lower()
        domains = []
        
        domain_keywords = {
            "Biologie/M√©decine": ["cell", "dna", "protein", "gene", "clinical", "patient", "medical", "health", "disease"],
            "Informatique/IA": ["algorithm", "computer", "software", "data", "network", "learning", "neural", "model", "system"],
            "Physique": ["quantum", "particle", "energy", "physics", "wave", "force", "atomic", "nuclear"],
            "Chimie": ["chemical", "molecule", "reaction", "compound", "atomic", "organic", "synthesis"],
            "Math√©matiques": ["equation", "theorem", "function", "mathematical", "calculation", "formula", "proof"],
            "Ing√©nierie": ["engineering", "design", "system", "structure", "material", "mechanical", "electrical"],
            "Sciences Sociales": ["social", "behavior", "psychological", "society", "human", "cultural", "economic"]
        }
        
        for domain, keywords in domain_keywords.items():
            keyword_count = sum(1 for keyword in keywords if keyword in text_lower)
            if keyword_count >= 2:  # Au moins 2 mots cl√©s du domaine
                domains.append(domain)
                
        return domains if domains else ["Sciences G√©n√©rales"]

    def scrape_web_content(self, url: str) -> str:
        """Scraping web optimis√© pour articles scientifiques"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            }
            
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Nettoyage sp√©cifique aux articles scientifiques
            for element in soup(["script", "style", "nav", "header", "footer", "aside", "menu", "button"]):
                element.decompose()
            
            # Strat√©gies d'extraction pour contenu scientifique
            content_selectors = [
                'article', '.article-content', '.research-paper', '.scientific-content',
                '.paper-body', '.main-content', '[role="main"]', '.content'
            ]
            
            content = None
            for selector in content_selectors:
                content = soup.select_one(selector)
                if content:
                    break
            
            if not content:
                content = soup.find('body')
            
            text = content.get_text(strip=True, separator=' ')
            text = re.sub(r'\s+', ' ', text)
            
            # V√©rification de la qualit√© du contenu
            if len(text.split()) < 100:
                return f"Contenu insuffisant extrait de {url} (seulement {len(text.split())} mots)"
            
            return text
            
        except Exception as e:
            return f"Erreur scraping: {str(e)}"

    def _fallback_scientific_summary(self, text: str, language: str) -> Dict:
        """R√©sum√© de fallback pour articles scientifiques"""
        processed = self.preprocess_scientific_text(text)
        summary = self._summarize_by_sections(processed)
        
        return {
            "summary": summary,
            "sections_analyzed": ["fallback"],
            "original_metrics": processed["metrics"],
            "summary_type": "fallback",
            "processing_strategy": "fallback_extraction"
        }

    def _fallback_translation(self, text: str, target_lang: str) -> str:
        """Traduction de fallback"""
        return f"[{target_lang}] {text}"

    def cleanup_memory(self):
        """Nettoyage m√©moire agressif pour documents longs"""
        try:
            # Lib√©ration des mod√®les de la m√©moire GPU
            for model in self.loaded_models.values():
                if hasattr(model, 'cpu'):
                    model.cpu()
                elif hasattr(model, 'model') and hasattr(model.model, 'cpu'):
                    model.model.cpu()
            
            # Nettoyage GPU
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # Nettoyage m√©moire Python
            gc.collect()
            
            logger.info("üßπ M√©moire nettoy√©e pour traitement de documents longs")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Nettoyage m√©moire partiel: {e}")

    def get_model_status(self) -> Dict:
        """Retourne le statut des mod√®les scientifiques"""
        return {
            "models_loaded": self.models_loaded,
            "device": str(self.device),
            "loaded_models": list(self.loaded_models.keys()),
            "max_context_length": 16384,  # LED
            "supported_languages": list(set([lang for lang in self.language_codes.keys() if len(lang) > 2])),
            "specialized_for": "scientific_articles_long_documents"
        }


# Instance globale sp√©cialis√©e
scientific_processor = ScientificArticleProcessor()

# Fonction de compatibilit√©
def get_multilingual_models(device="auto"):
    return ScientificArticleProcessor(device=device)

if __name__ == "__main__":
    # Test avec un texte scientifique long
    processor = ScientificArticleProcessor()
    print("üî¨ Statut:", processor.get_model_status())
    
    # Test de r√©sum√© scientifique
    scientific_text = """
    Abstract: This comprehensive study investigates the impact of deep learning architectures on medical image analysis. 
    We evaluated convolutional neural networks (CNNs) and transformer-based models across multiple medical imaging modalities.
    
    Introduction: Medical image analysis has undergone significant transformation with the advent of deep learning. 
    Traditional machine learning approaches are increasingly being replaced by sophisticated neural networks.
    
    Methodology: We conducted a systematic review of 250 peer-reviewed studies from 2018 to 2024. 
    The analysis included CT scans, MRI images, and histological samples across various medical conditions.
    
    Results: Our findings demonstrate that transformer-based models achieve 23% higher accuracy in anomaly detection 
    compared to traditional CNNs. However, computational requirements remain a significant challenge.
    
    Discussion: The superior performance of attention-based architectures suggests potential for clinical deployment, 
    though interpretability and computational efficiency require further investigation.
    
    Conclusion: Deep learning continues to revolutionize medical imaging, with transformer models showing particular promise 
    for complex diagnostic tasks. Future work should focus on model optimization and clinical validation.
    """
    
    result = processor.summarize_long_article(
        scientific_text, 
        language="anglais",
        summary_type="structured"
    )
    
    print(f"üìä M√©triques: {result['original_metrics']}")
    print(f"üìù R√©sum√©: {result['summary'][:500]}...")
    
    # Test de m√©tadonn√©es
    metadata = processor.extract_scientific_metadata(scientific_text)
    print(f"üîç M√©tadonn√©es: {metadata}")