import logging
import torch
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    MBart50TokenizerFast,
    MBartForConditionalGeneration,
    MarianMTModel,
    MarianTokenizer
)
import gc

logger = logging.getLogger(__name__)


class MultilingualModels:
    """
    Version optimis√©e pour SummarEase Pro avec lazy-loading et gestion m√©moire.
    """

    def __init__(self, device="cpu"):
        self.device = device
        self.loaded = False

        # Containers pour mod√®les et tokenizers
        self.models = {}
        self.tokenizers = {}

        logger.info("üß† MultilingualModels initialis√© (Lazy-loading activ√©)")

    def _load_specific_model(self, model_name):
        """Charge un mod√®le sp√©cifique de mani√®re optimis√©e"""
        try:
            if model_name == "barthez":
                self.tokenizers["barthez"] = AutoTokenizer.from_pretrained("moussaKam/barthez-orangesum-abstract")
                self.models["barthez"] = AutoModelForSeq2SeqLM.from_pretrained(
                    "moussaKam/barthez-orangesum-abstract"
                ).to(self.device)

            elif model_name == "bart_en":
                self.tokenizers["bart_en"] = AutoTokenizer.from_pretrained("facebook/bart-large-cnn")
                self.models["bart_en"] = AutoModelForSeq2SeqLM.from_pretrained(
                    "facebook/bart-large-cnn"
                ).to(self.device)

            elif model_name == "mt5":
                self.tokenizers["mt5"] = AutoTokenizer.from_pretrained("google/mt5-small")
                self.models["mt5"] = AutoModelForSeq2SeqLM.from_pretrained(
                    "google/mt5-small"
                ).to(self.device)

            elif model_name == "mbart":
                self.tokenizers["mbart"] = MBart50TokenizerFast.from_pretrained(
                    "facebook/mbart-large-50-many-to-many-mmt"
                )
                self.models["mbart"] = MBartForConditionalGeneration.from_pretrained(
                    "facebook/mbart-large-50-many-to-many-mmt"
                ).to(self.device)

            elif model_name == "m2m":
                self.tokenizers["m2m"] = AutoTokenizer.from_pretrained("facebook/m2m100_418M")
                self.models["m2m"] = AutoModelForSeq2SeqLM.from_pretrained(
                    "facebook/m2m100_418M"
                ).to(self.device)

            elif model_name == "marian_fr_en":
                self.tokenizers["marian_fr_en"] = MarianTokenizer.from_pretrained(
                    "Helsinki-NLP/opus-mt-fr-en"
                )
                self.models["marian_fr_en"] = MarianMTModel.from_pretrained(
                    "Helsinki-NLP/opus-mt-fr-en"
                ).to(self.device)

            elif model_name == "marian_en_fr":
                self.tokenizers["marian_en_fr"] = MarianTokenizer.from_pretrained(
                    "Helsinki-NLP/opus-mt-en-fr"
                )
                self.models["marian_en_fr"] = MarianMTModel.from_pretrained(
                    "Helsinki-NLP/opus-mt-en-fr"
                ).to(self.device)

            logger.info(f"‚úÖ Mod√®le {model_name} charg√© avec succ√®s")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du chargement du mod√®le {model_name}: {str(e)}")
            return False

    def load_essential_models(self):
        """Charge uniquement les mod√®les essentiels pour r√©duire l'empreinte m√©moire"""
        if self.loaded:
            return True
            
        logger.info("üöÄ Chargement des mod√®les essentiels...")
        
        essential_models = ["barthez", "bart_en", "m2m"]
        success_count = 0
        
        for model_name in essential_models:
            if self._load_specific_model(model_name):
                success_count += 1
        
        self.loaded = success_count > 0
        logger.info(f"‚úÖ {success_count}/{len(essential_models)} mod√®les essentiels charg√©s")
        return self.loaded

    def load_all(self):
        """Charge tous les mod√®les (m√©thode originale pr√©serv√©e)"""
        if self.loaded:
            return True
            
        logger.info("üöÄ Chargement de tous les mod√®les...")
        
        all_models = [
            "barthez", "bart_en", "mt5", "mbart", 
            "m2m", "marian_fr_en", "marian_en_fr"
        ]
        success_count = 0
        
        for model_name in all_models:
            if self._load_specific_model(model_name):
                success_count += 1
        
        self.loaded = success_count > 0
        logger.info(f"‚úÖ {success_count}/{len(all_models)} mod√®les charg√©s")
        return self.loaded

    def get_model_status(self):
        return {
            "models_loaded": self.loaded,
            "device": self.device,
            "loaded_models_count": len(self.models)
        }

    def summarize_text(self, text, lang="fran√ßais", length="moyen"):
        """R√©sume le texte avec gestion d'erreurs am√©lior√©e"""
        if not self.loaded:
            if not self.load_essential_models():
                raise Exception("Impossible de charger les mod√®les essentiels")

        try:
            # Validation de la longueur du texte - CORRIG√â : caract√®res au lieu de mots
            if len(text.strip()) < 50:
                raise ValueError("Le texte est trop court pour √™tre r√©sum√© (minimum 50 caract√®res)")
            
            # S√©lection du mod√®le adapt√©
            if lang == "fran√ßais":
                model_key = "barthez"
            elif lang == "anglais":
                model_key = "bart_en"
            else:
                model_key = "mt5"  # Mod√®le multilingue par d√©faut

            # Chargement √† la demande si n√©cessaire
            if model_key not in self.models:
                if not self._load_specific_model(model_key):
                    # Fallback vers un mod√®le disponible
                    if "barthez" in self.models:
                        model_key = "barthez"
                    elif "mt5" in self.models:
                        model_key = "mt5"
                    else:
                        raise Exception("Aucun mod√®le de r√©sum√© disponible")

            model = self.models[model_key]
            tokenizer = self.tokenizers[model_key]

            # Configuration de la longueur du r√©sum√©
            max_len = 120 if length == "court" else 220 if length == "moyen" else 350

            # Tokenization avec gestion des textes longs
            inputs = tokenizer(
                text, 
                return_tensors="pt", 
                truncation=True, 
                max_length=1024
            ).to(self.device)
            
            # G√©n√©ration du r√©sum√© - SUPPRIMER early_stopping qui cause des warnings
            output = model.generate(
                **inputs, 
                max_length=max_len,
                num_beams=4
                # early_stopping=True  # Supprim√© car cause des warnings
            )

            return tokenizer.decode(output[0], skip_special_tokens=True)
            
        except Exception as e:
            logger.error(f"Erreur lors du r√©sum√©: {str(e)}")
            raise

    def translate_text(self, text, src_lang, tgt_lang):
        """Traduit le texte avec gestion d'erreurs am√©lior√©e"""
        if not self.loaded:
            if not self.load_essential_models():
                raise Exception("Impossible de charger les mod√®les essentiels")

        try:
            # Validation
            if src_lang == tgt_lang:
                return text  # Pas de traduction n√©cessaire

            # S√©lection du mod√®le de traduction
            if src_lang == "fran√ßais" and tgt_lang == "anglais":
                model_key = "marian_fr_en"
            elif src_lang == "anglais" and tgt_lang == "fran√ßais":
                model_key = "marian_en_fr"
            else:
                model_key = "m2m"  # Mod√®le multilingue

            # Chargement √† la demande si n√©cessaire
            if model_key not in self.models:
                if not self._load_specific_model(model_key):
                    raise Exception(f"Mod√®le de traduction {src_lang}->{tgt_lang} non disponible")

            tok = self.tokenizers[model_key]
            mod = self.models[model_key]

            # Configuration sp√©cifique pour m2m
            if model_key == "m2m":
                lang_map = {
                    "fran√ßais": "fr", "anglais": "en", "espagnol": "es", 
                    "allemand": "de", "arabe": "ar"
                }
                tok.src_lang = lang_map.get(src_lang, "fr")

            # Tokenization et traduction
            inputs = tok(text, return_tensors="pt", truncation=True, max_length=512).to(self.device)
            out = mod.generate(**inputs)

            return tok.decode(out[0], skip_special_tokens=True)
            
        except Exception as e:
            logger.error(f"Erreur lors de la traduction {src_lang}->{tgt_lang}: {str(e)}")
            raise

    def cleanup_memory(self):
        """Nettoie la m√©moire GPU et lib√®re les ressources"""
        try:
            # Lib√©ration des mod√®les
            for model in self.models.values():
                if hasattr(model, 'cpu'):
                    model.cpu()
            
            # Nettoyage GPU
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # Nettoyage m√©moire Python
            gc.collect()
            
            logger.info("üßπ M√©moire nettoy√©e avec succ√®s")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Impossible de nettoyer compl√®tement la m√©moire: {str(e)}")


def get_multilingual_models(device="cpu"):
    return MultilingualModels(device=device)